import{_ as a,a as t,b as i}from"./chunks/tabular_as_input.DKyoDHVV.js";import{_ as s,c as o,a3 as n,o as l}from"./chunks/framework.CIdFYEl6.js";const r="/assets/jiangyanginppt.DhOlE4n2.png",u="/assets/read_doc.BshYkHP-.png",w=JSON.parse('{"title":"AI Large Model Integration Feature Usage Guide","description":"","frontmatter":{},"headers":[],"relativePath":"guide/plugin_llm.md","filePath":"guide/plugin_llm.md","lastUpdated":1748334019000}'),p={name:"guide/plugin_llm.md"};function d(g,e,h,c,m,b){return l(),o("div",null,e[0]||(e[0]=[n('<h1 id="ai-large-model-integration-feature-usage-guide" tabindex="-1">AI Large Model Integration Feature Usage Guide <a class="header-anchor" href="#ai-large-model-integration-feature-usage-guide" aria-label="Permalink to &quot;AI Large Model Integration Feature Usage Guide&quot;">​</a></h1><p>Mogan does not support large model integration. However, our semi-open source <a href="https://liiistem.com/" target="_blank" rel="noreferrer">Liii STEM</a> has built-in large model plugins.</p><h2 id="text-math-tables-etc-as-input" tabindex="-1">Text, Math, Tables, etc. as Input <a class="header-anchor" href="#text-math-tables-etc-as-input" aria-label="Permalink to &quot;Text, Math, Tables, etc. as Input&quot;">​</a></h2><h3 id="inserting-a-large-model-block" tabindex="-1">Inserting a Large Model Block <a class="header-anchor" href="#inserting-a-large-model-block" aria-label="Permalink to &quot;Inserting a Large Model Block&quot;">​</a></h3><p>You can insert a large model block (Session) by following the diagram below: <img src="'+a+'" alt="llm_session"></p><p>Most web-based large models only support plain text input. However, in Liii STEM, you can use <strong>any</strong> rendered format as input for the large model.Your input will be highlighted in blue when sent to the model. As shown below:</p><p><img src="'+t+'" alt="input_in_rendered_format"></p><p>This feature supports not only mathematical formulas but also tables, as shown below:</p><p><img src="'+i+'" alt="table"></p><h2 id="editable-llm-output" tabindex="-1">Editable LLM Output <a class="header-anchor" href="#editable-llm-output" aria-label="Permalink to &quot;Editable LLM Output&quot;">​</a></h2><p>A common issue when using large models is organizing, archiving, and asking follow-up questions based on the model&#39;s responses.</p><p>The output of the large model integrated into Liii STEM can be rendered directly within Liii STEM, allowing you to edit and modify it easily. As shown below, you can use Liii STEM to automatically generate quizzes and homework answers:</p><p><img src="'+r+'" alt="simple"></p><h2 id="using-files-as-llm-input" tabindex="-1">Using Files as LLM Input <a class="header-anchor" href="#using-files-as-llm-input" aria-label="Permalink to &quot;Using Files as LLM Input&quot;">​</a></h2><p>When using large models, we often need to analyze or process existing documents. Liii STEM supports reading files directly as input content for the LLM.</p><h3 id="usage" tabindex="-1">Usage <a class="header-anchor" href="#usage" aria-label="Permalink to &quot;Usage&quot;">​</a></h3><p>Use the following syntax to include a file:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%include /path/to/your/file</span></span></code></pre></div><p>For example: <img src="'+u+'" alt="read_doc"></p><p>Note: The path supports absolute paths. The file content will automatically be used as the input content for the LLM.</p><h2 id="faq" tabindex="-1">FAQ <a class="header-anchor" href="#faq" aria-label="Permalink to &quot;FAQ&quot;">​</a></h2><p>Q: Can I use Chinese for questions and answers? A: Yes, you need to set the document language to Chinese.</p>',22)]))}const L=s(p,[["render",d]]);export{w as __pageData,L as default};
